<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Thinking generatively | An introduction to generative thinking and probabilistic methods for consulting</title>
  <meta name="description" content="Chapter 2 Thinking generatively | An introduction to generative thinking and probabilistic methods for consulting" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Thinking generatively | An introduction to generative thinking and probabilistic methods for consulting" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="hendersontrent/bayes-for-consulting" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Thinking generatively | An introduction to generative thinking and probabilistic methods for consulting" />
  
  
  

<meta name="author" content="Trent Henderson" />


<meta name="date" content="2021-03-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An introduction to generative thinking and probabilistic methods for consulting</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="thinking-generatively.html"><a href="thinking-generatively.html"><i class="fa fa-check"></i><b>2</b> Thinking generatively</a><ul>
<li class="chapter" data-level="2.1" data-path="thinking-generatively.html"><a href="thinking-generatively.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>2.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="2.2" data-path="thinking-generatively.html"><a href="thinking-generatively.html#a-short-psychological-example-adult-satisfaction-with-their-job"><i class="fa fa-check"></i><b>2.2</b> A short psychological example: Adult satisfaction with their job</a><ul>
<li class="chapter" data-level="2.2.1" data-path="thinking-generatively.html"><a href="thinking-generatively.html#plotting-our-best-guess-at-the-process"><i class="fa fa-check"></i><b>2.2.1</b> Plotting our best guess at the process</a></li>
<li class="chapter" data-level="2.2.2" data-path="thinking-generatively.html"><a href="thinking-generatively.html#plotting-a-random-sample-of-actual-data"><i class="fa fa-check"></i><b>2.2.2</b> Plotting a random sample of actual data</a></li>
<li class="chapter" data-level="2.2.3" data-path="thinking-generatively.html"><a href="thinking-generatively.html#combining-our-best-guess-and-the-observed-random-sample-of-data-for-an-informed-estimate-of-the-population-process"><i class="fa fa-check"></i><b>2.2.3</b> Combining our best guess and the observed random sample of data for an informed estimate of the population process</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="thinking-generatively.html"><a href="thinking-generatively.html#the-essence-of-bayesian-statistics"><i class="fa fa-check"></i><b>2.3</b> The essence of Bayesian statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="thinking-generatively.html"><a href="thinking-generatively.html#taking-uncertainty-quantification-a-step-further"><i class="fa fa-check"></i><b>2.3.1</b> Taking uncertainty quantification a step further</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><i class="fa fa-check"></i><b>3</b> When the normal distribution is inappropriate (generalised linear models)</a><ul>
<li class="chapter" data-level="3.1" data-path="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html#poisson"><i class="fa fa-check"></i><b>3.1</b> Poisson</a></li>
<li class="chapter" data-level="3.2" data-path="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>3.2</b> Negative binomial</a></li>
<li class="chapter" data-level="3.3" data-path="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html#gamma"><i class="fa fa-check"></i><b>3.3</b> Gamma</a></li>
<li class="chapter" data-level="3.4" data-path="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html"><a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html#beta"><i class="fa fa-check"></i><b>3.4</b> Beta</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><i class="fa fa-check"></i><b>4</b> A structured Bayesian workflow for program evaluation and management consulting</a><ul>
<li class="chapter" data-level="4.1" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#use-qualitative-project-components-to-develop-priors"><i class="fa fa-check"></i><b>4.1</b> 1. Use qualitative project components to develop priors</a><ul>
<li class="chapter" data-level="4.1.1" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#how-to-elicit-priors-from-interviews-of-qualitative-reading"><i class="fa fa-check"></i><b>4.1.1</b> How to elicit priors from interviews of qualitative reading</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#specify-appropriately-uncertain-priors-for-your-context-despite-purported-stakeholder-certainty"><i class="fa fa-check"></i><b>4.2</b> 2. Specify appropriately uncertain priors for your context, despite purported stakeholder certainty</a></li>
<li class="chapter" data-level="4.3" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#think-generatively-about-the-underlying-statistical-process-of-interest"><i class="fa fa-check"></i><b>4.3</b> 3. Think generatively about the underlying statistical process of interest</a></li>
<li class="chapter" data-level="4.4" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#test-competing-models-using-loo-cv-and-k-fold-cv"><i class="fa fa-check"></i><b>4.4</b> 4. Test competing models using LOO-CV and K-Fold-CV</a><ul>
<li class="chapter" data-level="4.4.1" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>4.4.1</b> Leave-one-out cross-validation</a></li>
<li class="chapter" data-level="4.4.2" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>4.4.2</b> K-Fold cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#emphasise-credible-intervals-over-point-estimates-in-outputs-and-graphics"><i class="fa fa-check"></i><b>4.5</b> 5. Emphasise credible intervals over point estimates in outputs and graphics</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#credible-intervals-versus-confidence-intervals"><i class="fa fa-check"></i><b>4.5.1</b> Credible intervals versus confidence intervals</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#uncertainty-visualisation-example-1-bayesian-posterior-distributions"><i class="fa fa-check"></i><b>4.5.2</b> Uncertainty Visualisation Example 1: Bayesian posterior distributions</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#uncertainty-visualisation-example-2-frequentist-logistic-regression"><i class="fa fa-check"></i><b>4.5.3</b> Uncertainty Visualisation Example 2: Frequentist logistic regression</a></li>
<li class="chapter" data-level="4.5.4" data-path="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html"><a href="a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html#other-useful-uncertainty-visualisation-resources"><i class="fa fa-check"></i><b>4.5.4</b> Other useful uncertainty visualisation resources</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/hendersontrent/bayes-for-consulting" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An introduction to generative thinking and probabilistic methods for consulting</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="thinking-generatively" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Thinking generatively</h1>
<p>The normal distribution is the only type of distribution I learnt in my formal psychology education. Everything had to be normal - from distributions of quantitative variable values to all the assumptions made in statistical tests. There was no indication at all that other probability distributions existed, let alone their immense utility and appropriateness in many circumstances. Further, emphasis was very heavy on the <em>summative</em> parts of analysis, that is, that certain aspects of our model diagnostics must be normally distributed. But no focus was given to the important concept of the <strong>underlying process</strong> - the statistical process which generated the data we have seen. Indeed, this means many models in psychological research are purely a means to an end and based on the limited tools that are taught. I would not be surprised to hear sentiment from students such as ‘I need to remove univariate outliers because they violate normality’.</p>
<p>Two much more important questions arise out of this scenario:</p>
<ol style="list-style-type: decimal">
<li>Is it possible that these ‘outliers’ are actually part of the population you are drawing from? (Given you sampled them, chances of this being yes are probably very high, meaning you have strong theoretical grounds to <strong>not</strong> remove them - it’s more likely your model is inappropriate)</li>
<li>Are you sure that the underlying process generating your observed data is <em>actually</em> [Gaussian]? (<a href="https://en.wikipedia.org/wiki/Gaussian_function" class="uri">https://en.wikipedia.org/wiki/Gaussian_function</a>?) What evidence do you have that suggests this explicitly?</li>
</ol>
<p>These questions require a different type of thinking that could be called <em>generative</em> as it considers the core of all statistics - the actual data generation process. But this line of thinking is not taught in psychology at university, the focus is so much on the tools themselves in <code>SPSS</code> or other software and how the outputs are likely to be, rather than the underlying statistical mechanics of our areas of interest. As I was writing this chapter, a <a href="https://twitter.com/betanalpha/status/1361778324519997440">tweet</a> by the fantastic <a href="https://betanalpha.github.io/">Michael Betancourt</a> summarised the issue well. Essentially, instead of worrying about how the effects will look in your model outputs, you should consider modelling the process that generated the data you observed/sampled. You can use the mathematics of <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> functions to work through it.</p>
<p>There are several reasons that may underpin this lack of sound statistical thinking, but I’ll just address a core one here - the absence of mathematical rigour. I did not see the formula for the normal distribution (or another other, obviously) in my psychology classes. So students blindly fit models that assume normality for various aspects of their design without understanding how they actually work. Let’s take standard maximum likelihood linear regression for example.</p>
<p>Most students would be able to recite the obvious: instead of fitting ordinary least-squares, maximum likelihood <em>maximises</em> the likelihood function. But most students would not then be able to tell you what the likelihood function does. What it does is find the parameters of your probability density function (in this case, the normal distribution) that maximise the overall likelihood (more specifically, the negative log-likelihood, but more on this later) of observing your data. For the normal distribution, these parameters are just the <code>mean</code> and <code>standard deviation</code>. This <a href="https://en.wikipedia.org/wiki/Frequentist_inference">frequentist</a> interpretation means that we assume the parameters themselves are fixed values that we are trying to estimate through our model, while the data itself is random. Further, if we collect enough data (i.e. run the study many more times, therefore sampling closer and closer to the whole population), we will eventually approximate the <em>true</em> parameters values that are fixed.</p>
<p>Since the normal distribution is fully specified by its mean (<span class="math inline">\(\mu\)</span>) and standard deviation (<span class="math inline">\(\sigma^{2}\)</span>), we can write this as shorthand:</p>
<p><span class="math inline">\(X \sim \mathcal{N}(\mu,\,\sigma^{2})\)</span></p>
<p>The full equation for normal distribution is:</p>
<p><span class="math inline">\(f(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}}\)</span></p>
<p>The math may look intense if you haven’t come across many equations in your studies, but all we are doing in practice is inserting values for <code>x</code> (the observed data), <span class="math inline">\(\mu\)</span> (the mean), and <span class="math inline">\(\sigma\)</span> (the standard deviation). Simple, right? Visually, it looks like this:</p>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>With this in mind, let’s quickly revisit what maximum likelihood is doing. It is finding the values for the mean and standard deviation that across all your observed datapoints, maximise the overall output value of the function. Luckily, modern software such as <code>R</code>, <code>Python</code>, and <code>Julia</code> (among many others) use nonlinear optimisers to find these values for us very quickly. It would take a long time to test many different values for both and determine the optimal ones. Now that we understand how the backbone of almost all models in frequentist statistics work, we can start to link back to this notion of <em>thinking generatively</em>. If you truly believe the process generating your observed data is Gaussian, then two questions should come to mind:</p>
<ol style="list-style-type: decimal">
<li>What do I believe the true mean value might be? How certain am I of this?</li>
<li>What do I believe the true standard deviation value might be? How certain am I of this?</li>
</ol>
<p>We are now at the crux of statistical thinking! You are now reasoning deeply about the appropriateness of model parameters and probability distributions that may have generated the data you have seen. This thinking takes you beyond the immediate concern of what might pop out of your model in an <a href="https://www.ibm.com/au-en/analytics/spss-statistics-software"><code>SPSS</code></a> or <code>R</code> summary output, and gets you thinking about the much more important questions.</p>
<p>Let’s take a look at an example to visualise what’s going on. We are going to be using the <code>R</code> programming language. If you are unfamiliar with <code>R</code>, it is an open-source programming language that is very powerful and flexible, and is the primary language used in the statistical and econometric sciences, among many others. <code>R</code> has a lot of fantastic and intuitive data wrangling, statistical analysis, and data visualisation toolboxes that are far more extensive, reproducible, and free compared to the software that dominates psychology, such as <code>SPSS</code> and <code>MPlus</code> and paid add-ons such as <code>AMOS</code>.</p>
<p>If you come from an <code>SPSS</code> (or other) background and have never coded before, fear not! I firmly believe anyone can become a programmer and now is definitely the time to start. As this resource is not an “introduction to programming” text, there is some assumed knowledge. If anything does not make sense, please search the piece of code or topic (especially on <a href="https://stackoverflow.com">Stack Overflow</a>) and try and figure it out. This is how most of us solve problems on real projects and simply knowing what or how to search for information and solutions is a critical skill in any programmer or statistician’s toolkit. Another excellent resource is <a href="https://r4ds.had.co.nz/">R for Data Science by Hadley Wickham</a>. If you cannot find the answer, though, feel free to send me a message (details at the bottom of this chapter).</p>
<div id="installing-r-and-rstudio" class="section level2">
<h2><span class="header-section-number">2.1</span> Installing R and RStudio</h2>
<p>You can download <code>R</code> <a href="https://www.r-project.org/">here</a> and RStudio <a href="https://rstudio.com/products/rstudio/download/">here</a>. RStudio is an integrated development environment for writing the <code>R</code> language, exploring datasets, inspecting graphics, and managing version control of code. Install <code>R</code> first, then RStudio. After that, you probably only ever need to open RStudio.</p>
</div>
<div id="a-short-psychological-example-adult-satisfaction-with-their-job" class="section level2">
<h2><span class="header-section-number">2.2</span> A short psychological example: Adult satisfaction with their job</h2>
<p>In this example, we are going to synthetically construct a basic research question that is somewhat related to psychology, and explore a generative approach to reasoning about it. The research premise is:</p>
<p><strong>Premise: We are interested in understanding what proportion of working adults are satisfied with their jobs.</strong></p>
<p>This overall premise is somewhat similar to the type of topic that a psychology university student might examine for an Honours thesis or other small research project for a course they might take.</p>
<div id="plotting-our-best-guess-at-the-process" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Plotting our best guess at the process</h3>
<p>We often cannot sample an entire population or know the <em>true</em> actual population parameters. This is entirely the point of <a href="https://en.wikipedia.org/wiki/Statistical_inference">statistical inference</a>. However, more often than not, there is a plethora of research available that we often trawl through when constructing literature reviews and new research questions. How can we more effectively put this to use? We can use this existing information to construct a <em>prior</em> understanding of what we think is going on <em>before</em> we observe any data.</p>
<p>Let’s say previous research suggests that the true proportion is somewhere around 0.7, but we aren’t entirely certain. We can use a probability density function to model this best guess with some relative uncertainty around it. In practice, this means the areas of highest probability density (i.e. occurring) will be 0.7 and the immediate proportions around it, while proportions further away from 0.7 will have lower probability density. When your outcome measure of interest is a proportion, a <a href="https://en.wikipedia.org/wiki/Beta_distribution"><code>beta distribution</code></a> is the distribution you should use to model this. We’ll also take this opportunity to load all the packages we need for the entire book.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="thinking-generatively.html#cb1-1"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb1-2"><a href="thinking-generatively.html#cb1-2"></a><span class="kw">library</span>(tibble)</span>
<span id="cb1-3"><a href="thinking-generatively.html#cb1-3"></a><span class="kw">library</span>(magrittr)</span>
<span id="cb1-4"><a href="thinking-generatively.html#cb1-4"></a><span class="kw">library</span>(tidyr)</span>
<span id="cb1-5"><a href="thinking-generatively.html#cb1-5"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb1-6"><a href="thinking-generatively.html#cb1-6"></a><span class="kw">library</span>(scales)</span>
<span id="cb1-7"><a href="thinking-generatively.html#cb1-7"></a><span class="kw">library</span>(brms)</span>
<span id="cb1-8"><a href="thinking-generatively.html#cb1-8"></a><span class="kw">library</span>(bayesplot)</span>
<span id="cb1-9"><a href="thinking-generatively.html#cb1-9"></a><span class="kw">library</span>(loo)</span>
<span id="cb1-10"><a href="thinking-generatively.html#cb1-10"></a><span class="kw">library</span>(haven)</span>
<span id="cb1-11"><a href="thinking-generatively.html#cb1-11"></a><span class="kw">library</span>(janitor)</span>
<span id="cb1-12"><a href="thinking-generatively.html#cb1-12"></a></span>
<span id="cb1-13"><a href="thinking-generatively.html#cb1-13"></a><span class="co"># A colour palette for consistent plotting</span></span>
<span id="cb1-14"><a href="thinking-generatively.html#cb1-14"></a></span>
<span id="cb1-15"><a href="thinking-generatively.html#cb1-15"></a>my_palette &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;1. Prior&quot;</span> =<span class="st"> &quot;#003f5c&quot;</span>,</span>
<span id="cb1-16"><a href="thinking-generatively.html#cb1-16"></a>                <span class="st">&quot;2. Random Sample of Adults (Likelihood)&quot;</span> =<span class="st"> &quot;#ffa600&quot;</span>,</span>
<span id="cb1-17"><a href="thinking-generatively.html#cb1-17"></a>                <span class="st">&quot;3. Unstandardised Posterior&quot;</span> =<span class="st"> &quot;#bc5090&quot;</span>,</span>
<span id="cb1-18"><a href="thinking-generatively.html#cb1-18"></a>                <span class="st">&quot;4. Standardised Posterior&quot;</span> =<span class="st"> &quot;#ff6361&quot;</span>)</span>
<span id="cb1-19"><a href="thinking-generatively.html#cb1-19"></a></span>
<span id="cb1-20"><a href="thinking-generatively.html#cb1-20"></a><span class="co"># Simulate data</span></span>
<span id="cb1-21"><a href="thinking-generatively.html#cb1-21"></a></span>
<span id="cb1-22"><a href="thinking-generatively.html#cb1-22"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">11</span>)</span>
<span id="cb1-23"><a href="thinking-generatively.html#cb1-23"></a></span>
<span id="cb1-24"><a href="thinking-generatively.html#cb1-24"></a>pr &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x,</span>
<span id="cb1-25"><a href="thinking-generatively.html#cb1-25"></a>                 <span class="dt">y =</span> <span class="kw">dbeta</span>(x, <span class="dt">shape1 =</span> <span class="fl">6.5</span>, <span class="dt">shape2 =</span> <span class="fl">3.5</span>),</span>
<span id="cb1-26"><a href="thinking-generatively.html#cb1-26"></a>                 <span class="dt">category =</span> <span class="st">&quot;1. Prior&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1-27"><a href="thinking-generatively.html#cb1-27"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> y <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(y))</span>
<span id="cb1-28"><a href="thinking-generatively.html#cb1-28"></a></span>
<span id="cb1-29"><a href="thinking-generatively.html#cb1-29"></a><span class="co"># Plot it</span></span>
<span id="cb1-30"><a href="thinking-generatively.html#cb1-30"></a></span>
<span id="cb1-31"><a href="thinking-generatively.html#cb1-31"></a>pr <span class="op">%&gt;%</span></span>
<span id="cb1-32"><a href="thinking-generatively.html#cb1-32"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">colour =</span> category)) <span class="op">+</span></span>
<span id="cb1-33"><a href="thinking-generatively.html#cb1-33"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb1-34"><a href="thinking-generatively.html#cb1-34"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Our prior&quot;</span>,</span>
<span id="cb1-35"><a href="thinking-generatively.html#cb1-35"></a>       <span class="dt">x =</span> <span class="st">&quot;Possible Values of Actual Proportion&quot;</span>,</span>
<span id="cb1-36"><a href="thinking-generatively.html#cb1-36"></a>       <span class="dt">y =</span> <span class="st">&quot;Probability Density&quot;</span>,</span>
<span id="cb1-37"><a href="thinking-generatively.html#cb1-37"></a>       <span class="dt">colour =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb1-38"><a href="thinking-generatively.html#cb1-38"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb1-39"><a href="thinking-generatively.html#cb1-39"></a><span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> my_palette) <span class="op">+</span></span>
<span id="cb1-40"><a href="thinking-generatively.html#cb1-40"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The y values in this graph represent <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability densities</a>. If we take any area between two x values on this graph, we then have the <em>probability</em> that a value lies within. Probability densities by themselves are not probabilities, but they help us understand where most of the data points lie in the underlying distribution (higher probability density = higher concentration of data points).</p>
<p>I’ll leave the true purpose of this initial prior understanding until after we discuss the more familiar <em>likelihood</em> - or as we now know it - the probability distribution that best describes the data we have seen based on an estimated mean and standard deviation.</p>
</div>
<div id="plotting-a-random-sample-of-actual-data" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Plotting a random sample of actual data</h3>
<p>Now let’s say we sampled 10 adults students and observed whether they were satisfied with their job or not and 5 said they were. We can then calculate the probability of this occurring.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="thinking-generatively.html#cb2-1"></a><span class="co"># The observed data</span></span>
<span id="cb2-2"><a href="thinking-generatively.html#cb2-2"></a></span>
<span id="cb2-3"><a href="thinking-generatively.html#cb2-3"></a>lh &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb2-4"><a href="thinking-generatively.html#cb2-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">dbinom</span>(<span class="dt">x =</span> x, <span class="dt">prob =</span> <span class="fl">0.5</span>, <span class="dt">size =</span> <span class="dv">10</span>),</span>
<span id="cb2-5"><a href="thinking-generatively.html#cb2-5"></a>         <span class="dt">category =</span> <span class="st">&quot;2. Random Sample of Adults (Likelihood)&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb2-6"><a href="thinking-generatively.html#cb2-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> x <span class="op">/</span><span class="st"> </span><span class="kw">max</span>(x))</span>
<span id="cb2-7"><a href="thinking-generatively.html#cb2-7"></a></span>
<span id="cb2-8"><a href="thinking-generatively.html#cb2-8"></a><span class="co"># Plot against our prior</span></span>
<span id="cb2-9"><a href="thinking-generatively.html#cb2-9"></a></span>
<span id="cb2-10"><a href="thinking-generatively.html#cb2-10"></a>pr <span class="op">%&gt;%</span></span>
<span id="cb2-11"><a href="thinking-generatively.html#cb2-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">colour =</span> category)) <span class="op">+</span></span>
<span id="cb2-12"><a href="thinking-generatively.html#cb2-12"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb2-13"><a href="thinking-generatively.html#cb2-13"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> lh, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb2-14"><a href="thinking-generatively.html#cb2-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Our prior and a random sample of students&quot;</span>,</span>
<span id="cb2-15"><a href="thinking-generatively.html#cb2-15"></a>       <span class="dt">x =</span> <span class="st">&quot;Possible Values of Actual Proportion&quot;</span>,</span>
<span id="cb2-16"><a href="thinking-generatively.html#cb2-16"></a>       <span class="dt">y =</span> <span class="st">&quot;Probability Density&quot;</span>,</span>
<span id="cb2-17"><a href="thinking-generatively.html#cb2-17"></a>       <span class="dt">colour =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb2-18"><a href="thinking-generatively.html#cb2-18"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb2-19"><a href="thinking-generatively.html#cb2-19"></a><span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> my_palette) <span class="op">+</span></span>
<span id="cb2-20"><a href="thinking-generatively.html#cb2-20"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="combining-our-best-guess-and-the-observed-random-sample-of-data-for-an-informed-estimate-of-the-population-process" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Combining our best guess and the observed random sample of data for an informed estimate of the population process</h3>
<p>Consistent with <a href="https://en.wikipedia.org/wiki/Probability_theory">probability theory</a>, we can multiple two probabilities (to account for both) to get their <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint probability</a>. This means we can multiply our prior by the likelihood to return something known as the <a href="https://en.wikipedia.org/wiki/Posterior_probability"><em>posterior</em></a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="thinking-generatively.html#cb3-1"></a><span class="co"># Multiply prior and likelihood to get the &#39;posterior&#39;</span></span>
<span id="cb3-2"><a href="thinking-generatively.html#cb3-2"></a></span>
<span id="cb3-3"><a href="thinking-generatively.html#cb3-3"></a>posterior &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x,</span>
<span id="cb3-4"><a href="thinking-generatively.html#cb3-4"></a>                        <span class="dt">y =</span> pr<span class="op">$</span>y<span class="op">*</span>lh<span class="op">$</span>y,</span>
<span id="cb3-5"><a href="thinking-generatively.html#cb3-5"></a>                        <span class="dt">category =</span> <span class="st">&quot;3. Unstandardised Posterior&quot;</span>)</span>
<span id="cb3-6"><a href="thinking-generatively.html#cb3-6"></a></span>
<span id="cb3-7"><a href="thinking-generatively.html#cb3-7"></a><span class="co"># Plot against our prior and the likelihood</span></span>
<span id="cb3-8"><a href="thinking-generatively.html#cb3-8"></a></span>
<span id="cb3-9"><a href="thinking-generatively.html#cb3-9"></a>pr <span class="op">%&gt;%</span></span>
<span id="cb3-10"><a href="thinking-generatively.html#cb3-10"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">colour =</span> category)) <span class="op">+</span></span>
<span id="cb3-11"><a href="thinking-generatively.html#cb3-11"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb3-12"><a href="thinking-generatively.html#cb3-12"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> lh, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb3-13"><a href="thinking-generatively.html#cb3-13"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> posterior, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb3-14"><a href="thinking-generatively.html#cb3-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Our prior, random sample, and posterior update&quot;</span>,</span>
<span id="cb3-15"><a href="thinking-generatively.html#cb3-15"></a>       <span class="dt">x =</span> <span class="st">&quot;Possible Values of Actual Proportion&quot;</span>,</span>
<span id="cb3-16"><a href="thinking-generatively.html#cb3-16"></a>       <span class="dt">y =</span> <span class="st">&quot;Probability Density&quot;</span>,</span>
<span id="cb3-17"><a href="thinking-generatively.html#cb3-17"></a>       <span class="dt">colour =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb3-18"><a href="thinking-generatively.html#cb3-18"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb3-19"><a href="thinking-generatively.html#cb3-19"></a><span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> my_palette) <span class="op">+</span></span>
<span id="cb3-20"><a href="thinking-generatively.html#cb3-20"></a><span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour =</span> <span class="kw">guide_legend</span>(<span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)) <span class="op">+</span></span>
<span id="cb3-21"><a href="thinking-generatively.html#cb3-21"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>As you can see, the posterior appears to resemble a sort of “trade-off” between our prior and the observed data. Building on this, so far we have looked at a sample of ten random adults. You might begin to wonder what the posterior distribution looks like if our sample size was larger but 50% still said they were satisfied with their job. Does our initial guess/intuition (the <em>prior</em>) become less important as we understand more of the actual population? Let’s take a look - we are going to write a function that we can reuse that only takes a sample size as an input.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="thinking-generatively.html#cb4-1"></a>do_bayes &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> <span class="dv">100</span>){</span>
<span id="cb4-2"><a href="thinking-generatively.html#cb4-2"></a>  </span>
<span id="cb4-3"><a href="thinking-generatively.html#cb4-3"></a>  <span class="co"># Prior</span></span>
<span id="cb4-4"><a href="thinking-generatively.html#cb4-4"></a>  </span>
<span id="cb4-5"><a href="thinking-generatively.html#cb4-5"></a>  x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb4-6"><a href="thinking-generatively.html#cb4-6"></a>  </span>
<span id="cb4-7"><a href="thinking-generatively.html#cb4-7"></a>  pr &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x,</span>
<span id="cb4-8"><a href="thinking-generatively.html#cb4-8"></a>                     <span class="dt">y =</span> <span class="kw">dbeta</span>(x, <span class="dt">shape1 =</span> <span class="fl">6.5</span>, <span class="dt">shape2 =</span> <span class="fl">3.5</span>),</span>
<span id="cb4-9"><a href="thinking-generatively.html#cb4-9"></a>                     <span class="dt">category =</span> <span class="st">&quot;1. Prior&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb4-10"><a href="thinking-generatively.html#cb4-10"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">y =</span> y <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(y))</span>
<span id="cb4-11"><a href="thinking-generatively.html#cb4-11"></a>  </span>
<span id="cb4-12"><a href="thinking-generatively.html#cb4-12"></a>  <span class="co"># Likelihood</span></span>
<span id="cb4-13"><a href="thinking-generatively.html#cb4-13"></a>  </span>
<span id="cb4-14"><a href="thinking-generatively.html#cb4-14"></a>  lh &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">0</span><span class="op">:</span>n) <span class="op">%&gt;%</span></span>
<span id="cb4-15"><a href="thinking-generatively.html#cb4-15"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">dbinom</span>(<span class="dt">x =</span> x, <span class="dt">prob =</span> <span class="fl">0.5</span>, <span class="dt">size =</span> n),</span>
<span id="cb4-16"><a href="thinking-generatively.html#cb4-16"></a>           <span class="dt">category =</span> <span class="st">&quot;2. Random Sample of Adults (Likelihood)&quot;</span>,</span>
<span id="cb4-17"><a href="thinking-generatively.html#cb4-17"></a>           <span class="dt">x =</span> x <span class="op">/</span><span class="st"> </span>n)</span>
<span id="cb4-18"><a href="thinking-generatively.html#cb4-18"></a>  </span>
<span id="cb4-19"><a href="thinking-generatively.html#cb4-19"></a>  <span class="co"># Posterior</span></span>
<span id="cb4-20"><a href="thinking-generatively.html#cb4-20"></a>  </span>
<span id="cb4-21"><a href="thinking-generatively.html#cb4-21"></a>  posterior &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x,</span>
<span id="cb4-22"><a href="thinking-generatively.html#cb4-22"></a>                          <span class="dt">y =</span> pr<span class="op">$</span>y<span class="op">*</span>lh<span class="op">$</span>y,</span>
<span id="cb4-23"><a href="thinking-generatively.html#cb4-23"></a>                          <span class="dt">category =</span> <span class="st">&quot;3. Unstandardised Posterior&quot;</span>)</span>
<span id="cb4-24"><a href="thinking-generatively.html#cb4-24"></a>  </span>
<span id="cb4-25"><a href="thinking-generatively.html#cb4-25"></a>  st_post &lt;-<span class="st"> </span>posterior <span class="op">%&gt;%</span></span>
<span id="cb4-26"><a href="thinking-generatively.html#cb4-26"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">y =</span> y <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(y),</span>
<span id="cb4-27"><a href="thinking-generatively.html#cb4-27"></a>           <span class="dt">category =</span> <span class="st">&quot;4. Standardised Posterior&quot;</span>)</span>
<span id="cb4-28"><a href="thinking-generatively.html#cb4-28"></a>  </span>
<span id="cb4-29"><a href="thinking-generatively.html#cb4-29"></a>  p &lt;-<span class="st"> </span>pr <span class="op">%&gt;%</span></span>
<span id="cb4-30"><a href="thinking-generatively.html#cb4-30"></a><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">colour =</span> category)) <span class="op">+</span></span>
<span id="cb4-31"><a href="thinking-generatively.html#cb4-31"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb4-32"><a href="thinking-generatively.html#cb4-32"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> lh, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb4-33"><a href="thinking-generatively.html#cb4-33"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> posterior, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb4-34"><a href="thinking-generatively.html#cb4-34"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> st_post, <span class="dt">size =</span> <span class="fl">1.25</span>) <span class="op">+</span></span>
<span id="cb4-35"><a href="thinking-generatively.html#cb4-35"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Our prior, random sample, and posterior update&quot;</span>,</span>
<span id="cb4-36"><a href="thinking-generatively.html#cb4-36"></a>         <span class="dt">subtitle =</span> <span class="kw">paste0</span>(<span class="st">&quot;N = &quot;</span>, n),</span>
<span id="cb4-37"><a href="thinking-generatively.html#cb4-37"></a>         <span class="dt">x =</span> <span class="st">&quot;Possible Values of Actual Proportion&quot;</span>,</span>
<span id="cb4-38"><a href="thinking-generatively.html#cb4-38"></a>         <span class="dt">y =</span> <span class="st">&quot;Probability Density&quot;</span>,</span>
<span id="cb4-39"><a href="thinking-generatively.html#cb4-39"></a>         <span class="dt">colour =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb4-40"><a href="thinking-generatively.html#cb4-40"></a><span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb4-41"><a href="thinking-generatively.html#cb4-41"></a><span class="st">    </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> my_palette) <span class="op">+</span></span>
<span id="cb4-42"><a href="thinking-generatively.html#cb4-42"></a><span class="st">    </span><span class="kw">guides</span>(<span class="dt">colour =</span> <span class="kw">guide_legend</span>(<span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)) <span class="op">+</span></span>
<span id="cb4-43"><a href="thinking-generatively.html#cb4-43"></a><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span>
<span id="cb4-44"><a href="thinking-generatively.html#cb4-44"></a>  </span>
<span id="cb4-45"><a href="thinking-generatively.html#cb4-45"></a>  <span class="kw">return</span>(p)</span>
<span id="cb4-46"><a href="thinking-generatively.html#cb4-46"></a>}</span>
<span id="cb4-47"><a href="thinking-generatively.html#cb4-47"></a></span>
<span id="cb4-48"><a href="thinking-generatively.html#cb4-48"></a>p_<span class="dv">100</span> &lt;-<span class="st"> </span><span class="kw">do_bayes</span>(<span class="dt">n =</span> <span class="dv">100</span>)</span>
<span id="cb4-49"><a href="thinking-generatively.html#cb4-49"></a><span class="kw">print</span>(p_<span class="dv">100</span>)</span></code></pre></div>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>As you can see, as sample size increases, the posterior distribution is driven more and more by the likelihood - meaning the prior (especially a ‘wide’ or <em>‘uninformative’</em> prior) becomes increasingly less important. Moreover, if we specify a very narrow/specifc prior (commonly referred to as <em>‘informative’</em>), this diminishing importance relative to the likelihood is not as strong as what we see here. In theory, when sample sizes are very large, the results produced by this line of thinking get closer and closer to those produced by the maximum-likelihood-driven analysis we are familiar with. More simply, as sample sizes increase, the posterior approximates the likelihood. Very interesting! Let’s try <code>N = 1,000</code> just to really drive this point home:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="thinking-generatively.html#cb5-1"></a>p_<span class="dv">1000</span> &lt;-<span class="st"> </span><span class="kw">do_bayes</span>(<span class="dt">n =</span> <span class="dv">1000</span>)</span>
<span id="cb5-2"><a href="thinking-generatively.html#cb5-2"></a><span class="kw">print</span>(p_<span class="dv">1000</span>)</span></code></pre></div>
<p><img src="01-thinking_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Evidently, at a sample size of <code>N = 1000</code> the likelihood and the standardised posterior basically fully overlap (which is why the likelihood visually appears mostly ‘hidden’). The other important thing that might stand out to you at this point is the inclusion of something called the <em>Standardised Posterior</em> which is calculated as the unstandardised posterior probability densities divided by the sum of all the probability densities. This ensures our posterior probabilities are <em>actual</em> probabilities (meaning they sum to 1). Let’s explore this in more detail.</p>
</div>
</div>
<div id="the-essence-of-bayesian-statistics" class="section level2">
<h2><span class="header-section-number">2.3</span> The essence of Bayesian statistics</h2>
<p>The steps we have just gone through are actually in fact the core of <a href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian statistics</a>. These three components, the prior, likelihood, and posterior (the combination of the two), are the essential parts of Bayes’ theorem. In order to turn this posterior into an actual probability distribution, we need to standardise our new estimates across all the probabilities of the data occurring, so we divide the prior-likelihood product by this <em>marginal likelihood</em> (see the “Standardised Posterior” in the previous example). This leaves us with the full Bayes’ theorem:</p>
<p><span class="math inline">\(P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}\)</span></p>
<p>Where:</p>
<p><span class="math inline">\(P(\theta \mid D)\)</span> - this is called the <strong>posterior</strong> (probability of model parameters given the data)</p>
<p><span class="math inline">\(P(D \mid \theta)\)</span> - this is called the <strong>likelihood</strong> (probability of the data given model parameters)</p>
<p><span class="math inline">\(P(\theta)\)</span> - this is called the <strong>prior</strong> (our expressed understanding of the probability of model parameters)</p>
<p><span class="math inline">\(P(D)\)</span> - this is called the <strong>marginal likelihood</strong> (probability of the data)</p>
<p>Let’s pause for a moment. We started out discussing regular old normal distributions and statistics, and ended up in the Bayesian world. How did this happen? Essentially, the likelihood is the nexus between both realms of statistics. In the frequentist world, the likelihood is the estimated function of interest, whereas in the Bayesian world, it forms one piece of the puzzle, allowing us to combine is with any prior knowledge and/or uncertainty we have. Very neat.</p>
<div id="taking-uncertainty-quantification-a-step-further" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Taking uncertainty quantification a step further</h3>
<p>Since we are in the mindset of generative thinking, we are not restricted to the single value guesses at the true population distribution mean and standard deviation. In Bayesian statistics, we can specify a distribution of potential values for each of the mean and standard deviation. Practically, if we assessed prior research and found that estimates of the mean varied by some amount, we might assume that the true value of the mean lies somewhere within a probability distribution of the values we found in prior research. We could calculate a mean and standard deviation of this collection of prior means, and feed that uncertainty in the model, giving us more flexibility over how we conceptualise the underlying statistical process and quantify our inherent uncertainty. We can of course do the same for the standard deviation. This can be thought of as <em>placing distributions on the parameters of distributions.</em></p>
<p><img src="https://i.imgflip.com/4yva3g.jpg" /></p>
<p>Using the power of modern statistical computing libraries such as <a href="https://mc-stan.org/"><code>Stan</code></a> that allow us to use algorithms such as <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo</a>, we can simulate the underlying statistical process and obtain probabilistic estimates of the parameters of interest, conditioned on the data we have observed. This means we are not restricted by the limiting assumptions of frequentist statistics which largely denotes that parameter values are fixed and the data is random. An explanation of Hamiltonian Monte Carlo is beyond the scope of this chapter, but for an intuitive and informal high-level understanding of what Hamiltonian Monte Carlo is doing, check out <a href="https://twitter.com/betanalpha/status/1351749413564256256">this Twitter thread</a> by Michael Betancourt or his <a href="https://arxiv.org/pdf/1701.02434.pdf">excellent paper</a> on the topic.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="when-the-normal-distribution-is-inappropriate-generalised-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
