[["index.html", "An introduction to generative thinking and probabilistic methods for consulting Chapter 1 Preface", " An introduction to generative thinking and probabilistic methods for consulting Trent Henderson 2021-02-24 Chapter 1 Preface Welcome! Thank you for checking out An introduction to generative thinking and probabilistic methods for consulting or as I like to call it - Trent’s ramblings on things he has observed in his not-many-years as a professional data scientist. Before we get into the exciting details, I just wanted to give you a little bit of context about the book and about me, as my goal is to have the contents resonate with everyone, but especially those working in a (management) consulting or program evaluation setting. The book initially started out being called Statistical shit they don’t teach you in psychology (but probably should), but I thought this to be a bit too clickbait-y for my liking. Plus, the content began to expand beyond just the topics I believe are missing in university education into more applied problems and situations I encountered at work. This led me to reconsider the framing and purpose of the entire thing. The initial title was conceived because I studied psychology at the Honours and Masters level. While these studies inevitably contributed to me being in the position I am in today (PhD student in complex systems statistics, data scientist at the management consulting firm Nous Group, and founder of Orbisant Analytics), I harbour a large amount of negative feelings towards what I studied. If I could do it all over again, I would study statistics and computer science or statistics and software engineering. Contrary to many of my peers, I loved the many research methodology and statistics courses I did in psychology, but it wasn’t until I started learning programming and reading books in other fields (statistics, econometrics, pure mathematics) that I realised all of the essential mathematical details were glossed over and passed off as ‘beyond the scope of the course’. This meant I had to self-teach myself all of the gory details if I wanted to build more detailed, appropriate, and complex models. I became known amongst my peers as the person who would go home after class and code all night, code all weekend, and read mathematics and statistics books and tutorials in my spare time. While this helped me bridge the gap I missed in my formal education to get me to where I am today, the process definitely made me realise how much of it could be taught at the undergraduate level, just like students in other fields get taught. Thus, this book was born to help fasttrack others along a similar journey. This book became a product not so much out of negativity anymore, but more out of hope. Hope that future students will engage with technical details earlier. Hope that lecturers and tutors can steer classes safely into the details, building intuition first and precision through mathematics after. Hope that research supervisors will not just let their students assume that everything is normally distributed when it is not and simply write this as a limitation of their analysis in the the dissertation’s discussion section. As a final note, if you have any questions, queries, or spot any errors, please don’t hesitate to email me, tweet me, or submit an issue on the GitHub repository for the book. The book is currently a living document - I will keep adding sections to it as I write them. With that off my chest, let’s go! "],["thinking-generatively.html", "Chapter 2 Thinking generatively 2.1 Installing R and RStudio 2.2 A short psychological example: Adult satisfaction with their job 2.3 Plotting our best guess at the process 2.4 Plotting a random sample of actual data 2.5 Combining our best guess and the observed random sample of data for an informed estimate of the population process 2.6 The essence of Bayesian statistics 2.7 Taking uncertainty quantification a step further", " Chapter 2 Thinking generatively The normal distribution is the only type of distribution I learnt in my formal psychology education. Everything had to be normal - from distributions of quantitative variable values to all the assumptions made in statistical tests. There was no indication at all that other probability distributions existed, let alone their immense utility and appropriateness in many circumstances. Further, emphasis was very heavy on the summative parts of analysis, that is, that certain aspects of our model diagnostics must be normally distributed. But no focus was given to the important concept of the underlying process - the statistical process which generated the data we have seen. Indeed, this means many models in psychological research are purely a means to an end and based on the limited tools that are taught. I would not be surprised to hear sentiment from students such as ‘I need to remove univariate outliers because they violate normality’. Two much more important questions arise out of this scenario: Is it possible that these ‘outliers’ are actually part of the population you are drawing from? (Given you sampled them, chances of this being yes are probably very high, meaning you have strong theoretical grounds to not remove them - it’s more likely your model is inappropriate) Are you sure that the underlying process generating your observed data is actually Gaussian What evidence do you have that suggests this explicitly? These questions require a different type of thinking that could be called generative as it considers the core of all statistics - the actual data generation process. But this line of thinking is not taught in psychology at university, the focus is so much on the tools themselves in SPSS or other software and how the outputs are likely to be, rather than the underlying statistical mechanics of our areas of interest. As I was writing this chapter, a tweet by the fantastic Michael Betancourt summarised the issue well. Essentially, the issue boils down to stopping worrying about how the effects will look in your model outputs and starting to think about trying to model the process that generated the data you observed/sampled from and using the mathematics of likelihood functions to work through it. There are several reasons that may underpin the lack of education in this type of thinking, but I’ll just address a core one here - the absence of mathematical rigour. I did not see the formula for the normal distribution (or another other, obviously) in my actual classes. This means students blindly go about fitting models that assume normality for various aspects of their design without understanding how they actually work. Let’s take standard maximum likelihood linear regression for example. Most students would be able to recite the obvious, that instead of fitting ordinary least-squares, maximum likelihood instead maximises the likelihood function. But most students would not then be able to tell you what the likelihood function does. What it does is find the parameters of your probability density function (in this case, the normal distribution) that maximise the overall likelihood (more specifically, the negative log-likelihood, but more on this later) of observing your data. For the normal distribution, these parameters are just the mean and standard deviation. This frequentist interpretation means that we assume the parameters themselves are fixed values that we are trying to estimate through our model, while the data itself is random. Further, if we collect enough data (i.e. run the study many more times, therefore sampling closer and closer to the whole population), we will eventually approximate the true parameters values that are fixed. Since the normal distribution is fully specified by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma^{2}\\)), we can write this as shorthand: \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})\\) The full equation for normal distribution is: \\(f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}}\\) While the math may look intense if you haven’t come across many equations in your studies, it’s important to realise that all we are doing in practice is insert values for x (the observed data), \\(\\mu\\) (the mean), and \\(\\sigma\\) (the standard deviation). Simple, right? With this in mind, let’s quickly revisit what maximum likelihood is doing. It is finding the values for the mean and standard deviation that across all your observed datapoints, maximise the overall output value of the function. Luckily, modern software such as R, Python, and Julia (among many others) use very fast nonlinear optimisers to find these values for us very quickly. It would take a long time to test many different values for both and determine the optimal ones. Now that we understand how the backbone of almost all models in frequentist statistics work, we can start to link back to this notion of thinking generatively. If you truly believe the process generating your observed data is Gaussian, then two questions should come to mind: What do I believe the true mean value might be? How certain am I of this? What do I believe the true standard deviation value might be? How certain am I of this? We are now at the crux of statistical thinking! You are now reasoning deeply about the appropriateness of model parameters and probability distributions that may have generated the data you have seen. This thinking takes you beyond the immediate concern of what might pop out of your model in an SPSS or R summary output, and gets you thinking about the much more important questions. Let’s take a look at an example to visualise what’s going on. We are going to be using the R programming language. If you are unfamiliar with R, it is an open-source programming language that is very powerful and flexible, and is the primary language used in the statistical and econometric sciences, among many others. R has a lot of fantastic and intuitive data wrangling, statistical analysis, and data visualisation toolboxes that are far more extensive, reproducible, and free compared to the software that dominates psychology, such as SPSS and MPlus and paid add-ons such as AMOS. If you come from an SPSS (or other) background and have never coded before, fear not! I firmly believe anyone can become a programmer and now is definitely the time to start. As this resource is not an “introduction to programming” text, there is some assumed knowledge. If anything does not make sense, please search the piece of code or topic (especially on Stack Overflow) and try and figure it out. This is how most of us solve problems on real projects and simply knowing what or how to search for information and solutions is a critical skill in any programmer or statistician’s toolkit. If you cannot find the answer, though, feel free to send me a message (details at the bottom of this chapter). 2.1 Installing R and RStudio You can download R here and RStudio here. RStudio is an integrated development environment for writing the R language, exploring datasets, inspecting graphics, and managing version control of code. Install R first, then RStudio. After that, you probably only ever need to open RStudio. 2.2 A short psychological example: Adult satisfaction with their job In this example, we are going to synthetically construct a basic research question that is somewhat related to psychology, and explore a generative approach to reasoning about it. The research premise is: Premise: We are interested in understanding what proportion of working adults are satisfied with their jobs. This overall premise is somewhat similar to the type of topic that a psychology university student might examine for an Honours thesis or other small research project for a course they might take. 2.3 Plotting our best guess at the process We often cannot sample an entire population or know the true actual population parameters. This is entirely the point of statistical inference. However, more often than not, there is a plethora of research available that we often trawl through when constructing literature reviews and new research questions. How can we more effectively put this to use? We can use this existing information to construct a prior understanding of what we think is going on before we observe any data. Let’s say previous research suggests that the true proportion is somewhere around 0.7, but we aren’t entirely certain. We can use a probability density function to model this best guess with some relative uncertainty around it. In practice, this means the areas of highest probability density (i.e. occurring) will be 0.7 and the immediate proportions around it, while proportions further away from 0.7 will have lower probability density. When your outcome measure of interest is a proportion, a beta distribution is the distribution you should use to model this. We’ll also take this opportunity to load all the packages we need for the entire book. library(dplyr) library(tibble) library(magrittr) library(tidyr) library(ggplot2) library(scales) library(brms) library(bayesplot) library(loo) library(haven) library(janitor) # A colour palette for consistent plotting my_palette &lt;- c(&quot;1. Prior&quot; = &quot;#003f5c&quot;, &quot;2. Random Sample of Adults (Likelihood)&quot; = &quot;#ffa600&quot;, &quot;3. Unstandardised Posterior&quot; = &quot;#bc5090&quot;, &quot;4. Standardised Posterior&quot; = &quot;#ff6361&quot;) # Simulate data x &lt;- seq(0, 1, length.out = 11) pr &lt;- data.frame(x = x, y = dbeta(x, shape1 = 6.5, shape2 = 3.5), category = &quot;1. Prior&quot;) %&gt;% mutate(y = y / sum(y)) # Plot it pr %&gt;% ggplot(aes(x = x, y = y, colour = category)) + geom_line(size = 1.25) + labs(title = &quot;Our prior&quot;, x = &quot;Possible Values of Actual Proportion&quot;, y = &quot;Probability Density&quot;, colour = NULL) + theme_bw() + scale_colour_manual(values = my_palette) + theme(legend.position = &quot;bottom&quot;) The y values in this graph represent probability densities. If we take any area between two x values on this graph, we then have the probability that a value lies within. Probability densities by themselves are not probabilities, but they help us understand where most of the data points lie in the underlying distribution (higher probability density = higher concentration of data points). I’ll leave the true purpose of this initial prior understanding until after we discuss the more familiar likelihood - or as we now know it - the probability distribution that best describes the data we have seen based on an estimated mean and standard deviation. 2.4 Plotting a random sample of actual data Now let’s say we sampled 10 adults students and observed whether they were satisfied with their job or not and 5 said they were. We can then calculate the probability of this occurring. # The observed data lh &lt;- data.frame(x = 0:10) %&gt;% mutate(y = dbinom(x = x, prob = 0.5, size = 10), category = &quot;2. Random Sample of Adults (Likelihood)&quot;) %&gt;% mutate(x = x / max(x)) # Plot against our prior pr %&gt;% ggplot(aes(x = x, y = y, colour = category)) + geom_line(size = 1.25) + geom_line(data = lh, size = 1.25) + labs(title = &quot;Our prior and a random sample of students&quot;, x = &quot;Possible Values of Actual Proportion&quot;, y = &quot;Probability Density&quot;, colour = NULL) + theme_bw() + scale_colour_manual(values = my_palette) + theme(legend.position = &quot;bottom&quot;) 2.5 Combining our best guess and the observed random sample of data for an informed estimate of the population process Consistent with probability theory, we can multiple two probabilities (to account for both) to get their joint probability. This means we can multiply our prior by the likelihood to return something known as the posterior. # Multiply prior and likelihood to get the &#39;posterior&#39; posterior &lt;- data.frame(x = x, y = pr$y*lh$y, category = &quot;3. Unstandardised Posterior&quot;) # Plot against our prior and the likelihood pr %&gt;% ggplot(aes(x = x, y = y, colour = category)) + geom_line(size = 1.25) + geom_line(data = lh, size = 1.25) + geom_line(data = posterior, size = 1.25) + labs(title = &quot;Our prior, random sample, and posterior update&quot;, x = &quot;Possible Values of Actual Proportion&quot;, y = &quot;Probability Density&quot;, colour = NULL) + theme_bw() + scale_colour_manual(values = my_palette) + guides(colour = guide_legend(nrow = 2, byrow = TRUE)) + theme(legend.position = &quot;bottom&quot;) As you can see, the posterior appears to resemble a sort of “trade-off” between our prior and the observed data. Building on this, so far we have looked at a sample of ten random adults. You might begin to wonder what the posterior distribution looks like if our sample size was larger but 50% still said they were satisfied with their job. Does our initial guess/intuition (the prior) become less important as we understand more of the actual population? Let’s take a look - we are going to write a function that we can reuse that only takes a sample size as an input. do_bayes &lt;- function(n = 100){ # Prior x &lt;- seq(0, 1, length.out = n+1) pr &lt;- data.frame(x = x, y = dbeta(x, shape1 = 6.5, shape2 = 3.5), category = &quot;1. Prior&quot;) %&gt;% mutate(y = y / sum(y)) # Likelihood lh &lt;- data.frame(x = 0:n) %&gt;% mutate(y = dbinom(x = x, prob = 0.5, size = n), category = &quot;2. Random Sample of Adults (Likelihood)&quot;, x = x / n) # Posterior posterior &lt;- data.frame(x = x, y = pr$y*lh$y, category = &quot;3. Unstandardised Posterior&quot;) st_post &lt;- posterior %&gt;% mutate(y = y / sum(y), category = &quot;4. Standardised Posterior&quot;) p &lt;- pr %&gt;% ggplot(aes(x = x, y = y, colour = category)) + geom_line(size = 1.25) + geom_line(data = lh, size = 1.25) + geom_line(data = posterior, size = 1.25) + geom_line(data = st_post, size = 1.25) + labs(title = &quot;Our prior, random sample, and posterior update&quot;, subtitle = paste0(&quot;N = &quot;, n), x = &quot;Possible Values of Actual Proportion&quot;, y = &quot;Probability Density&quot;, colour = NULL) + theme_bw() + scale_colour_manual(values = my_palette) + guides(colour = guide_legend(nrow = 2, byrow = TRUE)) + theme(legend.position = &quot;bottom&quot;) return(p) } p_100 &lt;- do_bayes(n = 100) print(p_100) As you can see, as sample size increases, the posterior distribution is driven more and more by the likelihood - meaning the prior (especially a ‘wide’ or ‘uninformative’ prior) becomes increasingly less important. Moreover, if we specify a very narrow/specifc prior (commonly referred to as ‘informative’), this diminishing importance relative to the likelihood is not as strong as what we see here. In theory, when sample sizes are very large, the results produced by this line of thinking get closer and closer to those produced by the maximum-likelihood-driven analysis we are familiar with. More simply, as sample sizes increase, the posterior approximates the likelihood. Very interesting! Let’s try N = 1,000 just to really drive this point home: p_1000 &lt;- do_bayes(n = 1000) print(p_1000) Evidently, at a sample size of N = 1000 the likelihood and the standardised posterior basically fully overlap (which is why the likelihood visually appears mostly ‘hidden’). The other important thing that might stand out to you at this point is the inclusion of something called the Standardised Posterior which is calculated as the unstandardised posterior probability densities divided by the sum of all the probability densities. This ensures our posterior probabilities are actual probabilities (meaning they sum to 1). Let’s explore this in more detail. 2.6 The essence of Bayesian statistics The steps we have just gone through are actually in fact the core of Bayesian statistics. These three components, the prior, likelihood, and posterior (the combination of the two), are the essential parts of Bayes’ theorem. In order to turn this posterior into an actual probability distribution, we need to standardise our new estimates across all the probabilities of the data occurring, so we divide the prior-likelihood product by this marginal likelihood (see the “Standardised Posterior” in the previous example). This leaves us with the full Bayes’ theorem: \\(P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{P(D)}\\) Where: \\(P(\\theta \\mid D)\\) - this is called the posterior (probability of model parameters given the data) \\(P(D \\mid \\theta)\\) - this is called the likelihood (probability of the data given model parameters) \\(P(\\theta)\\) - this is called the prior (our expressed understanding of the probability of model parameters) \\(P(D)\\) - this is called the marginal likelihood (probability of the data) Let’s pause for a moment. We started out discussing regular old normal distributions and statistics, and ended up in the Bayesian world. How did this happen? Essentially, the likelihood is the nexus between both realms of statistics. In the frequentist world, the likelihood is the estimated function of interest, whereas in the Bayesian world, it forms one piece of the puzzle, allowing us to combine is with any prior knowledge and/or uncertainty we have. Very neat. 2.7 Taking uncertainty quantification a step further Since we are in the mindset of generative thinking, we are not restricted to the single value guesses at the true population distribution mean and standard deviation. In Bayesian statistics, we can specify a distribution of potential values for each of the mean and standard deviation. Practically, if we assessed prior research and found that estimates of the mean varied by some amount, we might assume that the true value of the mean lies somewhere within a probability distribution of the values we found in prior research. We could calculate a mean and standard deviation of this collection of prior means, and feed that uncertainty in the model, giving us more flexibility over how we conceptualise the underlying statistical process and quantify our inherent uncertainty. We can of course do the same for the standard deviation. This can be thought of as placing distributions on the parameters of distributions. Using the power of modern statistical computing libraries such as Stan that allow us to use algorithms such as Hamiltonian Monte Carlo, we can simulate the underlying statistical process and obtain probabilistic estimates of the parameters of interest, conditioned on the data we have observed. This means we are not restricted by the limiting assumptions of frequentist statistics which largely denotes that parameter values are fixed and the data is random. An explanation of Hamiltonian Monte Carlo is beyond the scope of this chapter, but for an intuitive and informal high-level understanding of what Hamiltonian Monte Carlo is doing, check out this Twitter thread by Michael Betancourt or his excellent paper on the topic. "],["when-the-normal-distribution-is-inappropriate-generalised-linear-models.html", "Chapter 3 When the normal distribution is inappropriate (generalised linear models) 3.1 Poisson 3.2 Negative binomial 3.3 Gamma 3.4 Beta", " Chapter 3 When the normal distribution is inappropriate (generalised linear models) While the normal distribution is very useful and even pops up in nature more often than one might expect, it is rarely the case that our variable of interest takes on this form specifically. Most of the time, are response variable probably is better measured by another probability density function. We are going to take a look at some main ones here. 3.1 Poisson A common case encountered by psychologists and social science researchers is when the outcome variable is a non-zero integer, or a ‘count’. For example, you may wish to understand the relationship at the postcode level between socioeconomic variables (e.g. average household income) and the number of residents who receive a clinical diagnosis of depression. The equation for this would look the same as the linear regression examples provided earlier in the book, however, we have one small problem: We cannot have decimals of a person. This makes a probability density function inappropriate to understand count data. Further, if we ran a linear regression on this data, the output would be continuous with decimals which would render any predictions we wish to make about the number of persons with depression inaccurate. Extrapolating the linear regression would also yield negative values, which does not make sense. Instead, while the formula remains the same, we turn to a type of distribution that instead uses a probability mass function which is designed to work with count data. This distribution is known as the Poisson distribution. Whereas the normal distribution is parameterised by its mean and standard deviation, the Poisson distribution is specified by one parameter - lambda - which assumes that mean and variance are equal. The formula for a Poisson distribution is: \\(P(X=x) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}\\) We can sample from a Poisson distribution in R quite easily: pois_data &lt;- data.frame(x = seq(from = 1, to = 30, by = 1)) %&gt;% mutate(y = dpois(x, lambda = 3)) # Probability densities for the generated x variable values # Make a plot pois_data %&gt;% ggplot(aes(x = x, y = y)) + geom_line(size = 1) + labs(x = &quot;x&quot;, y = &quot;Probability density&quot;) + scale_x_continuous(limits = c(1,30)) + theme(panel.grid.minor = element_blank()) 3.2 Negative binomial While Poisson distributions are immensely useful, it is not always the case that the mean of our data is equivalent to the variance. Luckily, there is a non-negative integer probability distribution for when this is the case - the negative binomial. The negative binomial generalises the Poisson distribution to when mean and variance are not the same. This makes them useful for many problems in the social sciences. We can simulate a negative binomial distribution and have a look at it. The negative binomial distribution can be written as: \\(f(k;r,p) = \\frac{\\Gamma(r+k)}{k!\\Gamma(r)}p^{k}(1-p)^{r}\\) We can sample from a negative binomial distribution in R: nb_data &lt;- data.frame(x = seq(from = 1, to = 30, by = 1)) %&gt;% mutate(y = dnbinom(x, size = 2, prob = 0.2)) # Probability densities for the generated x variable values # Make a plot nb_data %&gt;% ggplot(aes(x = x, y = y)) + geom_line(size = 1) + labs(x = &quot;x&quot;, y = &quot;Probability density&quot;) + scale_x_continuous(limits = c(1,30)) + theme(panel.grid.minor = element_blank()) 3.3 Gamma Gamma distributions are another class of probability density function that have very high utility. Gamma values must be positive real numbers. Gamma distributions are typically specified in one of three ways: With a shape parameter k and a scale parameter theta With a shape parameter alpha = k and an inverse scale parameter beta = 1/theta (known as rate) With a shape parameter k and a mean parameter mu = ktheta = alpha/beta The gamma distribution can be written as (in the shape-scale format): \\(f(x;k,\\theta) = \\frac{x^{k-1}e^{-\\frac{x}{\\theta}}}{\\theta^{k}\\Gamma(k)}\\) We can sample from a gamma distribution in R: gamma_data &lt;- data.frame(x = seq(from = 0, to = 10, by = 0.05)) %&gt;% mutate(y = dgamma(x, shape = 1, scale = 2)) # Probability densities for the generated x variable values # Make a plot gamma_data %&gt;% ggplot(aes(x = x, y = y)) + geom_line(size = 1) + labs(x = &quot;x&quot;, y = &quot;Probability density&quot;) + scale_x_continuous(limits = c(0,10)) + theme(panel.grid.minor = element_blank()) 3.4 Beta Beta distributions are unique and useful - they are specifically designed for response variables that range between 0 and 1. For example, your response variable might be the proportion of people in a given region who have a postgraduate degree. The normal distribution (and by extension standard linear regression) is inappropriate here, as extrapolation would clearly lead to values that exceed 0 in the negative direction, and 1 in the positive direction. This would limit any predictive capability outside the immediate sample. The negative binomial distribution can be written as: \\(f(x;\\alpha,\\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}}\\) We can sample from a beta distribution in R: beta_data &lt;- data.frame(x = seq(from = 0, to = 1, by = 0.05)) %&gt;% mutate(y = dbeta(x,1,5)) # Probability densities for the generated x variable values # Make a plot beta_data %&gt;% ggplot(aes(x = x, y = y)) + geom_line(size = 1) + labs(x = &quot;x&quot;, y = &quot;Probability density&quot;) + scale_x_continuous(limits = c(0,1)) + theme(panel.grid.minor = element_blank()) "],["a-structured-bayesian-workflow-for-program-evaluation-and-management-consulting.html", "Chapter 4 A structured Bayesian workflow for program evaluation and management consulting 4.1 1. Use qualitative project components to develop priors 4.2 2. Specify appropriately uncertain priors for your context, despite purported stakeholder certainty 4.3 3. Think generatively about the underlying statistical process of interest 4.4 4. Test competing models using LOO-CV and K-Fold-CV 4.5 5. Emphasise credible intervals over point estimates in outputs and graphics", " Chapter 4 A structured Bayesian workflow for program evaluation and management consulting With a basic understanding of Bayesian statistics, its general workflow, and other statistical models and distributions, we can now take a look at the proposed structured workflow for Bayesian analysis on program evaluation and consulting projects. There are five key components to the process and each will be explored in turn below. Use qualitative project components to develop priors Specify appropriately uncertain priors for your context, despite purported stakeholder certainty Think generatively about the underlying statistical process of interest Test competing models using LOO-CV and K-Fold-CV Emphasise credible intervals over point estimates in outputs and graphics 4.1 1. Use qualitative project components to develop priors Every consulting and program evaluation project I have worked on or heard of has involved some amount of stakeholder engagement. Whether it is something as minor as a project kickoff meeting, or more extensive consultation through interviews and workshops, there is always an aspect of talking to stakeholders. However, the typical way this process is used in consulting is to “supplement” quantitative analysis with the findings from qualitative engagement, or to “contextualise” quantitative outputs. This means the quantitative and qualitative are often discretised and only ever thought about in conjunction when it comes time to produce a deliverable or synthesise the entire project into a set of core findings. What can we do to make this relationship between quantitative and qualitative better and more informative? We can build in our understanding of the likely relationships (obtained through stakeholder engagement or other reading) into our statistical models as priors. Note that this is only possible within the Bayesian framework, not the traditional frequentist lens. 4.1.1 How to elicit priors from interviews of qualitative reading Th thought of trying to make a quantitative prior probability distribution from information that may not even remotely involve a number can be daunting. However, it primarily rests on the analyst’s ability reason and think abstractly about the information they are hearing or reading. As an example, let’s say a stakeholder said the following statement to your project team in an interview: Stakeholder Statement 1: We have many different service providers who deliver the program. We believe, anecdotally, that quite a few perform much better than the others, while quite a few perform much worse. There is not a single number contained in the statement. But does that mean we cannot use the information in a statistical model? Certainly not. There are some key parts of the statement that we can extract and examine further: Quite a few perform much better than the others, while quite a few perform much worse We have many different service providers who deliver the program 4.1.1.1 Quote 1: Quite a few perform much better than the others, while quite a few perform much worse Let’s take a look at the first quote. If we think distributionally, this information is very useful. It suggests that when we are thinking about effects, most are somewhere in the middle, but there are a decent amount toward the extremes. In a regression context, which is most often the case in consulting/program evaluation, these effects usually relate to regression coefficients (the value that relates a change in predictor value with change in output value). The initial reaction might be to use a normal distribution to specify a prior for the coefficient. This might be sufficient in a lot of cases, but if we really pay attention to the quote, it suggests the tails of the coefficient distribution are likely “fatter” (i.e. multiple providers) at each end. This is not really congruent with the idea of the normal distribution. What can we use instead? We could use two distributions that a very similar to the normal, but which have fatter tails. These are the Student’s t distribution and the Cauchy distribution. We can graph their probability densities against the normal to demonstrate this shift in the tail probabilities visually: # Simulate some random datapoints and calculate probability densities for each distribution type tmp &lt;- data.frame(x = seq(-6, 6, length.out = 100)) %&gt;% mutate(normal = dnorm(x, mean = 0, sd = 1), student_5 = dt(x, df = 5), student_30 = dt(x, df = 30), cauchy = dcauchy(x, location = 0, scale = 1)) %&gt;% gather(key = dist_type, value = y, 2:5) %&gt;% mutate(dist_type = case_when( dist_type == &quot;normal&quot; ~ &quot;Normal&quot;, dist_type == &quot;student_5&quot; ~ &quot;Student&#39;s t (df = 5)&quot;, dist_type == &quot;student_30&quot; ~ &quot;Student&#39;s t (df = 30)&quot;, dist_type == &quot;cauchy&quot; ~ &quot;Cauchy&quot;)) # Plot it tmp %&gt;% ggplot(aes(x = x, y = y, colour = dist_type)) + geom_line(size = 1.25) + labs(title = &quot;Normal vs Student&#39;s t vs Cauchy&quot;, x = &quot;x&quot;, y = &quot;Probability Density&quot;, colour = NULL) + theme_bw() + scale_colour_manual(values = c(&quot;#003f5c&quot;, &quot;#7a5195&quot;, &quot;#ef5675&quot;, &quot;#ffa600&quot;)) + theme(legend.position = &quot;bottom&quot;) Evidently, either one of these new distributions might capture the subject matter expert’s information better than defaulting to a normal. We can see this as the tails of all the specified distributions sit “above” that of the normal’s - indicating higher probability density in the tails (i.e. the collection of extreme providers). In my code you can see I played around with the degrees of freedom parameter of the Student’s t distributions, but did not adjust any of the others too much. As the degrees of freedom approaches the sample size, the Student’s t distribution will approximate the normal distribution. I adjusted the df parameter mostly for visual illustration, but the process of choosing appropriate prior distribution parameters is a very important one. Let’s take a closer look. As we know, priors tend to be dominated by the likelihood when the sample size is large. We also know that this occurs especially when priors are vague (i.e. wide and “uninformative”). So if we set a normal prior with a wide standard deviation, it is likely that this will be dominated by the likelihood as it is not contributing much specific value compared to the (more likely) tighter likelihood distribution. Prior Specification Key Takeaway: If you have a lot of uncertainty or want the actual data/likelihood to drive the posterior, specify a wide prior As a useful one-stop-shop resource for choosing a distribution, the table below denotes a few high-level descriptions of values we may encounter in the real world and the probability distributions that might appropriately model it. This list is not exhaustive and should only serve as a starting point or primer to dig deeper. Further, there is no indication of appropriate distribution parameters as this is case-dependent. Table 4.1: Known value types and corresponding potential distributions Types of values Bernoulli Beta Cauchy Gamma Normal Ordered logistic Poisson Negative binomial Student’s t Positive only No No No Yes No No No No No Non-negative integer No No No No No No Yes Yes No Numeric between 0 and 1 No Yes No No No No No No No Numeric negative or positive No No Yes No Yes No No No Yes Binary categorical Yes No No No No No No No No Ordered categorical No No No No No Yes No No No Now let’s take a look at the second quote. 4.1.1.2 Quote 2: We have many different service providers who deliver the program Coming soon… 4.2 2. Specify appropriately uncertain priors for your context, despite purported stakeholder certainty In my consulting experience, I have encountered stakeholders who maintain a sense of certainty and are adamant about outcomes being X or something must be Y. While the above example showed that it is possible to elicit useful information for statistical models from consultations, it is important to temper this with an understanding that there is very likely quite a bit of error or uncertainty around these statements. It may be tempting to take stakeholder information and construct a very narrow prior, and in some cases this is appropriate, but in consulting and policy/program work, it is more often than not a very messy and complex landscape where quantification of uncertainty is much needed and very important. This means analysts need to be able to listen to, understand, and value stakeholder information, but also consider it alongside the statistical implications. 4.3 3. Think generatively about the underlying statistical process of interest We covered much of the detail in this component in an earlier chapter, but will discuss it briefly again here. The basic premise is to think in-depth about the statistical process that might generate the data you will see. This will govern the selection of probability density/probability mass function used which has major implications for the analysis. A sample line of questioning that an analyst may consider at this stage is presented below (we are going to assume the outcome is continuous for the example): Is my outcome categorical, ordinal or continuous? Does any evidence exist about the population distribution or distributions of other samples? Is my outcome likely to be roughly normally distributed in the population? Why/why not? Is there likely to be more edge/extreme cases than one would expect in a normal distribution? 4.4 4. Test competing models using LOO-CV and K-Fold-CV Cross-validation (CV) is a powerful out-of-sample-based tool for model selection and evaluation of model performance (Vehtari, Gelman, &amp; Gabry, 2016). At a high level, it essentially works by removing a select number of samples k many times and measuring predictive accuracy against this held-out test set of data. See this article by Vehtari (2017) for a great explanation and applied example of Bayesian model selection using cross-validation techniques. There are two main CV techniques used for evaluating Bayesian models: Leave-one-out cross-validation K-Fold cross-validation 4.4.1 Leave-one-out cross-validation Leave-one-out cross-validation (LOO-CV or just ‘LOO’) is the most common form of CV for Bayesian analysis. It essentially works by removing a single datapoint as the test validation set, and using the remaining datapoints as the train set. We can compare models using the loo package in R easily with the function loo_compare() (where loo1, loo2 etc are loo objects): library(loo) loo_compare(loo1,loo2) 4.4.2 K-Fold cross-validation K-Fold cross validation (K-Fold) is a technique commonly employed by Bayesian analysts when models are more complex and include random effects, typically at the individual level. Traditional LOO is inappropriate for these models, because the random effects term is highly sensitive to the removal of any observations. K-Fold gets around this problem by diving up the data into K number of subsets, and then refitting the model K times, each time leaving out a single subset. Luckily for us, the fantastic loo package in R allows for comparison of models evaluated using LOO and K-Fold using the same loo_compare() function seen in the LOO section above. 4.5 5. Emphasise credible intervals over point estimates in outputs and graphics The final component in the structured Bayesian workflow is the presentation of findings. This is the most important part of the process for convincing others (especially non-technical people) that your workflow is appropriate and more informative than the comparatively quicker and easier frequentist modelling equivalent. Further, good data visualisation and presentation of findings (i.e. tables and descriptions) are key to not misleading readers into a false sense of certainty - one that permeates typical consulting reports where p values are seen as the most important thing and values such as standard errors and confidence intervals are typically omitted. The single most useful thing an analyst can do after modelling is complete, is present appropriate uncertainty bounds around any statistical outputs. Within the Bayesian framework, these are known as credible intervals. 4.5.1 Credible intervals versus confidence intervals Credible intervals specify the range in which a parameter values lies with a specified probability (compared to confidence intervals which are not probability distributions). Instead, confidence intervals are based off the frequentist long-run frequency concept. This denotes that if the data were collected many more time (e.g. 100), the parameter value would lie in a given range 95 out of those 100 times (without much care to how far outside the range it was in the other 5). Remember that in Bayesian statistics we treat parameters as random (and data as fixed) values to be estimated, while frequentists assume parameters are fixed values (and data is random) and can be better understood by collecting more data. Much like confidence intervals, the choice of interval is rather arbitrary, but it is common to see combinations of 80%, 90%, and 95% in Bayesian analysis. Let’s fit some Bayesian (and frequentist) models and graph them in a few different styles so you can get a sense of what these visualisations might look like. Note that producing these graphics in standard consulting tools such as Microsoft Excel or Think-cell is non-trivial if not impossible (as well as not being reproducible). 4.5.2 Uncertainty Visualisation Example 1: Bayesian posterior distributions The intuition behind uncertainty visualisation is the most important part of this section, but please have a read through the data wrangling and modelling code to try and understand what I have done. This first chunk of code is simply loading in and preparing the data for modelling. We are going to be looking at the Australian Election Voter Study and using 2013 data to derive informative priors for analysis on 2016 data. This is an example of a project that is perfect for a Bayesian approach and one that many consulting firms might undertake, as many firms release annual reports/updates on key sectors they service. A lot of tutorials and texts would opt for a synthetic or simple example here, but I want to highlight in this example that in consulting/program evaluation, when it’s done well, so much work is performed on just the preprocessing before any analysis is conducted. I won’t explain too much of the specifics here as data wrangling is not the focus of this book, but please run each segment and explore what I have done if you are not familiar with any of what I have written. # Read in datafiles and reduce dataset size to columns of interest d_2013 &lt;- read_sav(&quot;data/2. Australian Election Study, 2013.sav&quot;) %&gt;% dplyr::select(c(b1, a1, h1, h2)) %&gt;% drop_na() d_2016 &lt;- read_sas(&quot;data/2. Australian Election Study, 2016.sas7bdat&quot;) %&gt;% dplyr::select(c(B1, A1, H1, H2)) %&gt;% drop_na() %&gt;% clean_names() # Function to recode parties so they are interpretable, and clean up other variables cleaner &lt;- function(data, year) { # Recode parties tmp &lt;- data %&gt;% mutate(b1 = as.numeric(b1)) %&gt;% filter(b1 != 999) %&gt;% filter(a1 != 999) %&gt;% filter(h1 != 999) %&gt;% filter(h2 != 999) %&gt;% filter(b1 != -1) %&gt;% mutate(b1_name = case_when( b1 == &quot;1&quot; ~ &quot;Liberal&quot;, b1 == &quot;2&quot; ~ &quot;Labor&quot;, b1 == &quot;3&quot; ~ &quot;National (Country) Party&quot;, b1 == &quot;4&quot; ~ &quot;Greens&quot;, b1 == &quot;5&quot; ~ &quot;Other party&quot;, b1 == &quot;6&quot; ~ &quot;No party&quot;)) # Calculate age and make response variable an ordered factor tmp1 &lt;- tmp %&gt;% mutate(age = year - h2) %&gt;% mutate(age &gt;= 18) %&gt;% mutate(a1 = factor(a1, levels = c(1,2,3,4), ordered = TRUE)) return(tmp1) } prep_2013 &lt;- cleaner(d_2013, 2013) prep_2016 &lt;- cleaner(d_2016, 2016) # Build 2013 model with vague and uninformative priors m1 &lt;- brm(a1 ~ b1 + h1 + h2, data = prep_2013, family = cumulative(&quot;logit&quot;), iter = 2000, chains = 3, seed = 123) # Visualise distribution of coefficient posteriors to get correct prior shape for 2016 as.data.frame(m1) %&gt;% clean_names() %&gt;% dplyr::select(c(b_b1, b_h1, b_h2)) %&gt;% gather(key = parameter, value = value, 1:3) %&gt;% ggplot(aes(x = value)) + geom_histogram(binwidth = 0.001, fill = &quot;#ff764a&quot;) + labs(title = &quot;Posterior distributions for 2013&quot;, x = &quot;Coefficient Value&quot;, y = &quot;Frequency&quot;) + theme_bw() + facet_wrap(~parameter) The distribution for the coefficient of b1 (party) seems approximately normal so we will use that for the 2016 model. The distribution for the coefficient of h1 (sex) has fatter tails than a normal distribution, so we will specify a Cauchy for this variable. The distribution for the coefficient of h2 (age) is a bit hard to determine visually with the scale of the other variables squishing it. Let’s plot it by itself to see. It’s mildly kurtotic, but largely it appears we have another normally-distributed coefficient posterior. as.data.frame(m1) %&gt;% clean_names() %&gt;% dplyr::select(c(b_b1, b_h1, b_h2)) %&gt;% gather(key = parameter, value = value, 1:3) %&gt;% filter(parameter == &quot;b_h2&quot;) %&gt;% ggplot(aes(x = value)) + geom_histogram(binwidth = 0.001, fill = &quot;#ff764a&quot;) + labs(title = &quot;Posterior distributions for 2013&quot;, x = &quot;Coefficient Value&quot;, y = &quot;Frequency&quot;) + theme_bw() + facet_wrap(~parameter) We can now dynamically extract the mean and standard deviation of each coefficient and build these into the 2016 model directly. # Extract model components to use as prior for 2016 priors &lt;- as.data.frame(m1) %&gt;% clean_names() %&gt;% summarise(beta_party_mean = mean(b_b1), beta_party_sd = sd(b_b1), beta_sex_mean = mean(b_h1), beta_sex_sd = sd(b_h1), beta_age_mean = mean(b_h2), beta_age_sd = sd(b_h2)) # Insert extracted values into priors for 2016 use party_prior &lt;- paste0(&quot;normal(&quot;,priors$beta_party_mean,&quot;,&quot;,priors$beta_party_sd,&quot;)&quot;) sex_prior &lt;- paste0(&quot;cauchy(&quot;,priors$beta_sex_mean,&quot;,&quot;,priors$beta_sex_sd,&quot;)&quot;) age_prior &lt;- paste0(&quot;normal(&quot;,priors$beta_age_mean,&quot;,&quot;,priors$beta_age_sd,&quot;)&quot;) # Fit 2016 model m2 &lt;- brm(a1 ~ b1 + h1 + h2, data = prep_2013, family = cumulative(&quot;logit&quot;), prior = c(set_prior(prior = party_prior, class = &quot;b&quot;, coef = &quot;b1&quot;), set_prior(prior = sex_prior, class = &quot;b&quot;, coef = &quot;h1&quot;), set_prior(prior = age_prior, class = &quot;b&quot;, coef = &quot;h2&quot;)), iter = 2000, chains = 3, seed = 123) Now that our model has run, we can evaluate how well it fit through a few key visualisations using the excellent bayesplot package. 4.5.2.1 Model Fit 1: Posterior predictive checks Posterior predictive checks (PPC) are a very useful tool for visually model fit and the model’s capacity to simulate the observed data. For an ordinal model, we can produce barplots rather than continuous distributions. Ideally, we want our simulated data yrep (dark blue point and bars) to be overlapping with the top of the real sample y bars. pp_check(m2, type = &quot;bars&quot;, nsamples = 100) + labs(title = &quot;Posterior predictive check&quot;, x = &quot;Interest in Politics&quot;, y = &quot;Count&quot;) We can also check the cumulative probability function. Ideally, for each level of interest in politics, we want our simulated data to track around but close to the real sample. pp_check(m2, type = &quot;ecdf_overlay&quot;, nsamples = 100) + labs(title = &quot;Posterior predictive check of cumulative probability function&quot;, x = &quot;Interest in Politics&quot;, y = &quot;Cumulative Probability&quot;) 4.5.2.2 Model Fit 2: LOO-CV We can visually inspect our model’s sensitivity using a LOO plot. We want no values to exceed 0.5 and definitely none to exceed 0.7. loo1 &lt;- loo(m2) plot(loo1) 4.5.2.3 Model Fit 3: Traceplots At a high level, we can check if the multiple chains mixed in one line of R code. We want these just to look like white noise, which they do here. color_scheme_set(&quot;mix-blue-pink&quot;) mcmc_trace(m2, facet_args = list(nrow = 2, labeller = label_parsed)) 4.5.2.4 Final posterior visualisation With model checks done, and since this section is about posterior distribution visualisation, here is how you can automate in a few-liner rather than manually building histograms like we did earlier. mcmc_areas(m2, regex_pars = c(&quot;b1&quot;, &quot;h1&quot;, &quot;h2&quot;), area_method = &quot;scaled height&quot;) + labs(title = &quot;Coefficient posterior distributions&quot;) 4.5.3 Uncertainty Visualisation Example 2: Frequentist logistic regression Uncertainty visualisation is not limited to Bayesian statistics. Many R programmers are likely familiar with the geom_smooth() function that enables automated plotting of regression models and their confidence intervals. We can produce a quick univariate graph to illustrate this very easily using the classic mtcars dataset: mtcars %&gt;% ggplot(aes(x = mpg, y = vs)) + geom_point(size = 2) + geom_smooth(formula = y ~ x, method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = TRUE) + theme_bw() + labs(x = &quot;Mile per Gallon&quot;, y = &quot;vs&quot;) This graph appropriately denotes the relative uncertainty associated with the change in miles per gallon and the corresponding change in vs. However, while a univariate case makes for an easy graph, a multivariate case is much more realistic, where the end graphic is likely a plot of the regression coefficients and corresponding confidence intervals. # Fit model m3 &lt;- glm(vs ~ mpg + cyl + disp, family = &quot;binomial&quot;, data = mtcars) # Extract coefficients and confidence intervals coefs &lt;- as.data.frame(coef(m3)) coefs &lt;- rownames_to_column(coefs, &quot;parameter&quot;) confs &lt;- as.data.frame(confint(m3)) %&gt;% rename(lower = &quot;2.5 %&quot;) %&gt;% rename(upper = &quot;97.5 %&quot;) confs &lt;- rownames_to_column(confs, &quot;parameter&quot;) outs &lt;- coefs %&gt;% left_join(confs, by = c(&quot;parameter&quot; = &quot;parameter&quot;)) # Exponentiate to convert log-odds to odds for easier interpretation odds &lt;- outs %&gt;% rename(coefficient = `coef(m3)`) %&gt;% mutate(coefficient = exp(coefficient), lower = exp(lower), upper = exp(upper)) %&gt;% filter(parameter != &quot;(Intercept)&quot;) # Plot it odds %&gt;% ggplot() + geom_segment(aes(x = lower, xend = upper, y = parameter, yend = parameter), size = 3, colour = &quot;#ff764a&quot;, alpha = 0.8) + geom_point(aes(x = coefficient, y = reorder(parameter, coefficient)), size = 4, colour = &quot;#003f5c&quot;) + geom_vline(xintercept = 1, lty = &quot;dashed&quot;, size = 1, colour = &quot;#374c80&quot;) + labs(title = &quot;Logistic regression model summary&quot;, subtitle = &quot;Points indicate mean estimate, bands indicate 95% confidence interval&quot;, x = &quot;Value&quot;, y = &quot;Parameter&quot;) + theme_bw() 4.5.4 Other useful uncertainty visualisation resources Many other great statisticians and researchers have produced guides and software packages for uncertainty visualisation. A few of these resources are listed below: tidybayes - R package for Bayesian analysis using the grammar of graphics approach Visualizing uncertainty - chapter from ‘Fundamentals of Data Visualisation’ by Claus O. Wilke Uncertain About Uncertainty: How Qualitative Expressions of Forecaster Confidence Impact Decision-Making With Uncertainty Visualizations - paper by Padilla, Powell, Kay, and Hullman (2021) And remember: it’s almost always a bad idea to turn off confidence/credible interval shading on regression plots (yes, the frequentist equivalents too). "]]
